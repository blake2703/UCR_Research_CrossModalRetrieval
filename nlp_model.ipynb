{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP2ibFKAVF6u8CxiywmO13I"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IRpEA5wy_qv7"
      },
      "outputs": [],
      "source": [
        "pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "-jv19YQWA26r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "5-BwFJ6SANvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d hsankesara/flickr-image-dataset"
      ],
      "metadata": {
        "id": "XeqlqlpTAQYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import *\n",
        "\n",
        "file_name = 'flickr-image-dataset.zip' #the file is your dataset exact name\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Files Extracted')"
      ],
      "metadata": {
        "id": "U__NZm9cAUta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "id": "1SMM5EU1CVM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "jko91FRwH3ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-text "
      ],
      "metadata": {
        "id": "v2Tenr7rIAa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-hub"
      ],
      "metadata": {
        "id": "9FkSKpIoIB5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas"
      ],
      "metadata": {
        "id": "mtpz7zgfMXgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install seaborn"
      ],
      "metadata": {
        "id": "UbpFQrMfMdD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn"
      ],
      "metadata": {
        "id": "SuPhzmBtMf-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install matplotlib"
      ],
      "metadata": {
        "id": "9Lsa4R9fMcBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "gczx7dhGMbHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy"
      ],
      "metadata": {
        "id": "Aq6DCIx6MaBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import contractions\n",
        "import string\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "ePMAi-ZgCROU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0 Get Data and Greet Data"
      ],
      "metadata": {
        "id": "aXhN2WhnIKJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dataset should be broken up into multiple datasets:\n",
        "- data_raw = first read in of our data, but will not be touched\n",
        "- data_copy = data_raw copied, which will be used to clean, feature engineer, and wrangle our data"
      ],
      "metadata": {
        "id": "-qXfwurYIL69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_raw = pd.read_csv('/content/flickr30k_images/results.csv', delimiter='|')\n",
        "data_copy = pd.read_csv('/content/flickr30k_images/results.csv', delimiter='|')"
      ],
      "metadata": {
        "id": "eaOLD1ShCRnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_raw.info()"
      ],
      "metadata": {
        "id": "BYpbVBRbCieS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We notice that we have a missing value in the comment section. This will be important to us as we clean our data later. The other two columns look like they do not have any missing data."
      ],
      "metadata": {
        "id": "qAxKGykVIOo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_raw.sample(10)"
      ],
      "metadata": {
        "id": "vV_p__AmCkYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our data is structured with three columns:\n",
        "- image_name: which is the unique tag of the image\n",
        "- comment_number: which is the specific comment number about the image\n",
        "- comment: this is the actual comment left about the image <br><br>\n",
        "\n",
        "We expect to see 5 comments per uniuqe image_id so let's make sure that is the case:"
      ],
      "metadata": {
        "id": "rA85o5u7IR9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_name_unique_vals = data_raw['image_name'].unique().tolist()\n",
        "print(\"Total length of unique values: \", len(image_name_unique_vals))\n",
        "print(\"Total length of data frame: \",  len(data_raw))\n",
        "print(\"Total length of unique values * 5: \", len(image_name_unique_vals) * 5)"
      ],
      "metadata": {
        "id": "R0l-jK-fCnpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based off our findings above, we can see that there are in fact 31,783 unique images in the dataset, corresponding to 158,915 in the total dataset, so in fact we do have 5 comments per image."
      ],
      "metadata": {
        "id": "5-6XQtdTIVHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 The 4 C's of Data Cleaning: Correction, Completing, Creating, and Converting <br><br>\n",
        "\n",
        "In this stage, we will clean our data by:\n",
        "1. correcting values and outliers \n",
        "2. completing missing information\n",
        "3. creating new features for analysis\n",
        "4. converting fields to the correct format for calculations and presentation."
      ],
      "metadata": {
        "id": "CFFDuYWcIVnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1.1 Correcting data"
      ],
      "metadata": {
        "id": "KoDKwDEPIitR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Columns with null values:')\n",
        "print(data_copy.isnull().sum())"
      ],
      "metadata": {
        "id": "FnekwrDZCpz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_copy.describe(include = 'all').transpose()"
      ],
      "metadata": {
        "id": "DB73EaPiCsH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's make sure that our columns are formatted correctly so we can pull data from them:"
      ],
      "metadata": {
        "id": "yXkUpXSYIluH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_copy.columns"
      ],
      "metadata": {
        "id": "hb7RFPgSCt6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_copy.columns = data_copy.columns.str.replace(' ', '')\n",
        "data_copy.columns"
      ],
      "metadata": {
        "id": "JOLrmSQyCvkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the comment_number issue because we should have 5 unique comment_numbers not 6:"
      ],
      "metadata": {
        "id": "ydj9y6qNIt7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comment_number_unique_vals = data_copy['comment_number'].unique().tolist()\n",
        "print(comment_number_unique_vals)"
      ],
      "metadata": {
        "id": "N2eXbC7ICxm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if this last item in the list above correlates with our missing value in the comment column because it seems to have a number and a sentence combined into one:"
      ],
      "metadata": {
        "id": "4m31hHujIwZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_copy[data_copy['comment'].isnull()]"
      ],
      "metadata": {
        "id": "zQWJCHE-CzZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter location to where the data is missing a value\n",
        "data_copy[data_copy['image_name'] == '2199200615.jpg']\n",
        "#Create a temporary variable to hold that row\n",
        "tmp = data_copy.iloc[19999]\n",
        "\n",
        "#Get the comment number column\n",
        "holder = tmp['comment_number']\n",
        "\n",
        "#Seperate the text from the digit and print\n",
        "result = ''.join([i for i in holder if not i.isdigit()])\n",
        "\n",
        "#Update the temporary variable row\n",
        "tmp['comment'] = result \n",
        "tmp['comment_number'] = '4'\n",
        "\n",
        "#Update our data copy row to reflect change\n",
        "data_copy.iloc[19999] = tmp\n",
        "print(data_copy.iloc[19999])"
      ],
      "metadata": {
        "id": "EfiVn7AZC08p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Updated Columns with null values:') \n",
        "print(data_copy.isnull().sum())"
      ],
      "metadata": {
        "id": "zxf37zQcC3aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comment_number_unique_vals = data_copy['comment_number'].unique().tolist()\n",
        "print(comment_number_unique_vals)"
      ],
      "metadata": {
        "id": "iaspqLEuC5N0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_copy['comment_number'] = data_copy['comment_number'].str.lstrip()"
      ],
      "metadata": {
        "id": "_OiQED2uC6vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comment_number_unique_vals = data_copy['comment_number'].unique().tolist()\n",
        "print(comment_number_unique_vals)"
      ],
      "metadata": {
        "id": "Aj_bphf4C8jQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should also clean our sentences:\n",
        "1. Remove punctuations\n",
        "    - Ex:\n",
        "        - i am happy. -> i am happy\n",
        "2. Lower case all the data\n",
        "    - Ex:\n",
        "        - I AM HAPPY -> i am happy\n",
        "3. Remove emojis\n",
        "    - Ex:\n",
        "        - 🫠 -> ''\n",
        "4. Remove contractions\n",
        "    - Ex:\n",
        "        - I didn't do well -> I did not do well\n",
        "5. Remove extra whitespace\n",
        "    - Ex:\n",
        "        - ' i am happy ' -> 'i am happy'\n",
        "6. Deal with stopwords\n",
        "    - In this case I am going to remove them. Stopwords are words that are automatically omitted from a computer-generated index.\n",
        "    - Ex:\n",
        "        - \"I\", \"a\", \"the\"\n",
        "    - We may want to consider adding our own stopwords we see a lot of to the mix\n",
        "\n"
      ],
      "metadata": {
        "id": "wXpCnou7I2Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_contractions(comment):\n",
        "    \"\"\"\n",
        "    This function will expand all contractions to get rid of them.\n",
        "\n",
        "    Args:\n",
        "        comment (object): A column in the dataframe that contains all the comments from each image\n",
        "\n",
        "    Returns:\n",
        "       Object: A fixed object that has all the punctuations taken out of the sentence\n",
        "    \"\"\"\n",
        "    comment = contractions.fix(comment)\n",
        "    return comment\n",
        "data_copy['comment'] = data_copy['comment'].apply(expand_contractions)"
      ],
      "metadata": {
        "id": "ABqVKlvwC96x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuations(comment):\n",
        "    \"\"\"\n",
        "    This function will go through all the comment rows and remove all punctuations.\n",
        "    \n",
        "    Args:\n",
        "        comment (object): A column in the dataframe that contains all the comments from each image\n",
        "\n",
        "    Returns:\n",
        "        Object: A fixed object that has all the punctuations taken out of the sentence\n",
        "    \"\"\"\n",
        "    for letter in string.punctuation:\n",
        "        comment = comment.replace(letter, '')\n",
        "    return comment\n",
        "#call function on dataframe to remove punctuations\n",
        "data_copy['comment'] = data_copy['comment'].apply(remove_punctuations)"
      ],
      "metadata": {
        "id": "IoYytJq4DAEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert all letters in the comment column to lowercase\n",
        "data_copy['comment'] = data_copy['comment'].str.lower()"
      ],
      "metadata": {
        "id": "rdrjW6NzDC75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove all emojis from the comment column\n",
        "data_copy.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))"
      ],
      "metadata": {
        "id": "TRk0krHeDEwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get rid of all whitespace in the comment column\n",
        "data_copy['comment'] = data_copy['comment'].str.strip()"
      ],
      "metadata": {
        "id": "UP2eAsnjDGKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's find out what the most common words are from each sentence:"
      ],
      "metadata": {
        "id": "aiPuxYIvI66O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(\" \".join(data_copy[\"comment\"]).split()).most_common(100)"
      ],
      "metadata": {
        "id": "6b08jh6SDI5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop = stopwords.words('english') #assign all stopwords to variable stop\n",
        "data_copy['comment'] = data_copy['comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) #remove stopwords"
      ],
      "metadata": {
        "id": "S5FxMo3jDKnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With stopwords removed, here are the most common words:"
      ],
      "metadata": {
        "id": "3S_9O9aBI_1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(\" \".join(data_copy[\"comment\"]).split()).most_common(100)"
      ],
      "metadata": {
        "id": "qhVfT3fxDNdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1.2 Creating"
      ],
      "metadata": {
        "id": "i08SFLQmJCsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will be using feature engineering:\n",
        "- Count number of characters\n",
        "- Count number of words\n",
        "- Count number of capital characters\n",
        "- Count number of capital words\n",
        "- Count number of punctuations\n",
        "- Count number of words in quotes\n",
        "- Count number of sentences\n",
        "- Count number of unique words\n",
        "- Count special characters\n",
        "- Count stopwords\n",
        "- Calculate average word length\n",
        "- Calculate average sentence length\n",
        "- Unique word count vs word count\n",
        "- Stopword count vs word count"
      ],
      "metadata": {
        "id": "HW7poM_SJD3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_chars(comment): \n",
        "    \"\"\"\n",
        "    This function will count the amount of characters in each comment\n",
        "\n",
        "    Args:\n",
        "        comment (object): A column in the dataframe that contains all the comments from each image\n",
        "\n",
        "    Returns:\n",
        "        Object: A fixed object that has all the punctuations taken out of the sentence\n",
        "    \"\"\"\n",
        "    return len(comment)\n",
        "\n",
        "\n",
        "\n",
        "def count_words(comment):\n",
        "    \"\"\"\n",
        "    This function will count the amount of words in each comment\n",
        "\n",
        "    Args:\n",
        "        comment (object): A column in the dataframe that contains all the comments from each image\n",
        "\n",
        "    Returns:\n",
        "        Object: A fixed object that has all the punctuations taken out of the sentence\n",
        "    \"\"\"\n",
        "    return len(comment.split()) #split the string on each space\n",
        "\n",
        "\n",
        "#need this line for some reason\n",
        "nltk.download('punkt')\n",
        "def count_sentences(comment):\n",
        "    \"\"\"\n",
        "    This function will count the amount of sentences in each comment\n",
        "\n",
        "    Args:\n",
        "        comment (object): A column in the dataframe that contains all the comments from each image\n",
        "\n",
        "    Returns:\n",
        "        Object: A fixed object that has all the punctuations taken out of the sentence\n",
        "    \"\"\"\n",
        "    return len(nltk.sent_tokenize(comment))\n",
        "\n",
        "\n",
        "def count_unique_words(comment):\n",
        "    \"\"\"\n",
        "    This function will count the amount of unique words in a comment\n",
        "\n",
        "    Args:\n",
        "        comment (object): A column in the dataframe that contains all the comments from each image\n",
        "\n",
        "    Returns:\n",
        "        Object: A fixed object that has all the punctuations taken out of the sentence\n",
        "    \"\"\"\n",
        "    return len(set(comment.split()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def calc_avg_word_length(num_chars, num_words):\n",
        "    \"\"\"\n",
        "    This function will calculate the average word length per sentence\n",
        "\n",
        "    Args:\n",
        "        num_chars (int): number of chars per word\n",
        "        num_words (int): number of words per sentence\n",
        "\n",
        "    Returns:\n",
        "        Float: the average word length per setence\n",
        "    \"\"\"\n",
        "    return num_chars/num_words\n",
        "\n",
        "\n",
        "def calc_avg_sentence_length(num_words, num_sentences):\n",
        "    \"\"\"\n",
        "    This function will calculate the average sentence length per comment\n",
        "\n",
        "    Args:\n",
        "        num_words (int): number of words per comment\n",
        "        num_sentences (int): number of sentences per comment\n",
        "\n",
        "    Returns:\n",
        "        Float: the average sentence length per comment\n",
        "    \"\"\"\n",
        "    return num_words/num_sentences\n",
        "\n",
        "def count_unique_words_vs_word_count(num_words, num_unique):\n",
        "    \"\"\"\n",
        "    This function will count the number of unique words vs the total words per sentence\n",
        "\n",
        "    Args:\n",
        "        num_words (int): number of words per sentence\n",
        "        num_unique (int): number of unique words per sentence\n",
        "\n",
        "    Returns:\n",
        "        Float: the ratio of unique words to total words\n",
        "    \"\"\"\n",
        "    return num_unique/num_words"
      ],
      "metadata": {
        "id": "sS-bQCfbDvr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_copy['char_count'] = data_copy['comment'].apply(lambda x : count_chars(x))\n",
        "data_copy['word_count'] = data_copy['comment'].apply(lambda x : count_words(x))\n",
        "data_copy['sentence_count'] = data_copy['comment'].apply(lambda x : count_sentences(x))\n",
        "data_copy['unique_word_count'] = data_copy['comment'].apply(lambda x : count_unique_words(x))\n",
        "data_copy['average_word_length'] = data_copy['char_count']/data_copy['word_count']\n",
        "data_copy['average_sentence_length'] = data_copy['word_count']/data_copy['sentence_count']\n",
        "data_copy['unique_words_vs_words'] = data_copy['unique_word_count']/data_copy['word_count']"
      ],
      "metadata": {
        "id": "Lzo-CfUeDzv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_copy.columns"
      ],
      "metadata": {
        "id": "VDKdVt4gD1uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0 EDA"
      ],
      "metadata": {
        "id": "J6TURym9JJpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1 Analyzing Character Counts"
      ],
      "metadata": {
        "id": "y9BeGNHRJLBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_hist(x_size, y_size, facecolor, x_axis, edgecolor, bar_color, x_label, y_label, title):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x_size (int): x_axis size for figsize\n",
        "        y_size (int): y_axis size for figsize\n",
        "        facecolor (string): Hex value for the facecolor of graph\n",
        "        x_axis (object): Column of the dataframe to graph\n",
        "        edgecolor (string): Hex value for the edge color of histogram bins\n",
        "        bar_color (string): Hex value for the bar color of the histogram\n",
        "        x_label (string): x_axis title\n",
        "        y_label (string): y_axis title\n",
        "        title (string): The title of the graph\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(x_size, y_size))\n",
        "    plt.rcParams['axes.facecolor'] = facecolor\n",
        "    plt.rcParams['figure.facecolor'] = facecolor\n",
        "    #using Sturge's rule to calculate number of bins\n",
        "    bin_count = int(np.ceil(np.log2(len(data_copy))) + 1)\n",
        "    values, bins, bars = plt.hist(x_axis, bin_count, edgecolor=edgecolor, facecolor=bar_color, alpha=1)\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.title(title)\n",
        "    # plt.bar_label(bars, fontsize=10, color='black')\n",
        "    plt.margins(x=0.01, y=0.1)\n",
        "    plt.grid(False)\n",
        "    print(\"Displaying histogram with \" + str(bin_count) + \" bins.\")"
      ],
      "metadata": {
        "id": "dzgY8yUQJSI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_hist(15,\n",
        "          10,\n",
        "          '#DEEBF7',\n",
        "          data_copy['char_count'],\n",
        "          'white',\n",
        "          '#3182BD',\n",
        "          'Char Count',\n",
        "          'Count',\n",
        "          'Char Count Distribution')"
      ],
      "metadata": {
        "id": "LCSHQOpPJUQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based off our historgram distribution we can see that a majority of our data lies in bin 3. We used Sturge's rule to calculate the number of bins, in which we use the formula, $$[log_{2}n + 1]$$. <br><br> This formula uses **n** which is the total number of observations in the dataset and takes the log of that and adds 1. <br><br> Since our data peaks in bin 3 we can say that most of our data will range from roughly 29-44 characters. Adding on, our data is skewed right as we see that there is a little bit of a tail to the right of the graph. This means that the mean is greater than the median."
      ],
      "metadata": {
        "id": "K8wRBCocJjNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1 Analyzing Word Counts"
      ],
      "metadata": {
        "id": "aWPZ-2FGJkzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_hist(15,\n",
        "          10,\n",
        "          '#DEEBF7',\n",
        "          data_copy['word_count'],\n",
        "          'white',\n",
        "          '#3182BD',\n",
        "          'Word Count',\n",
        "          'Count',\n",
        "          'Word Count Distribution')"
      ],
      "metadata": {
        "id": "MvkGZoYhJnPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.0 Model 1 (Baseline)\n",
        "1. Create a temporary dataframe in order to shrink our dataset down because it takes forever to train\n",
        "2. remove .jpg from image name column\n",
        "3. label each image\n",
        "4. split into validation and train datasets (must do this on every run in order to avoid overfitting)\n",
        "5. encode each sentence (give each character a token)\n",
        "6. build and run model\n",
        "\n",
        "How to improve?\n",
        "1. run more epochs\n",
        "2. train on more data\n",
        "3. clean data differently\n",
        "4. add more layers\n",
        "5. change activation function\n",
        "6. change hyperparameters"
      ],
      "metadata": {
        "id": "ysBjWvEbJNYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a temporary data frame for our model to run our testing on:"
      ],
      "metadata": {
        "id": "3zsGPQgoJpVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = data_copy"
      ],
      "metadata": {
        "id": "xZwk7ykCD5HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove '.jpg' from image\n",
        "tmp['image_name'] = tmp['image_name'].str.replace('.jpg', '')\n",
        "tmp['image_name'].astype(str)"
      ],
      "metadata": {
        "id": "i1b4p6bCEFg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#temporary df until final model is built\n",
        "temp33 = tmp\n",
        "dic = dict()\n",
        "label_list = list()\n",
        "#create new column label to label each picture to a numeric digit representing a class\n",
        "temp33['label'] = np.nan\n",
        "\n",
        "\n",
        "def classify_images(df):\n",
        "  '''\n",
        "    A function to turn the images to a label classifying them\n",
        "\n",
        "    Args: \n",
        "        df: a dataframe\n",
        "    \n",
        "'''\n",
        "    #temporary counting variable\n",
        "    temp_counter = 0\n",
        "    #loop through each image in the image_name column\n",
        "    for value in df['image_name']:\n",
        "        #assign the counter as the value while the image_name is the key\n",
        "        if value not in dic:\n",
        "            dic[value] = temp_counter\n",
        "            temp_counter+=1\n",
        "    #put the image_name column values in a list\n",
        "    images = temp33['image_name'].tolist()\n",
        "    #loop through the image list and append the correct counter\n",
        "    for image in images:\n",
        "        label_list.append(dic[image])\n",
        "    #update label column to the correct value\n",
        "    temp33['label'] = label_list\n",
        "\n",
        "classify_images(temp33)\n",
        "\n",
        "#make sure output is correct\n",
        "temp33.tail()"
      ],
      "metadata": {
        "id": "firtsHWsEHgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shrink dataset to test our baseline model:"
      ],
      "metadata": {
        "id": "MjVgZ2EoJy-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp33 = temp33[:500]"
      ],
      "metadata": {
        "id": "vzzZgULfELMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = tf.keras.utils.to_categorical(temp33[\"label\"].values, num_classes=100)\n",
        "x_train, x_test, y_train, y_test = train_test_split(temp33['comment'], y, test_size=0.30, random_state=101)"
      ],
      "metadata": {
        "id": "b99GDrriEQr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the pretrained model from tensorflow\n",
        "preprocessor = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
        "encoder = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base/1\")\n",
        "\n",
        "def get_embeddings(sentences):\n",
        "  '''return BERT-like embeddings of input text\n",
        "  Args:\n",
        "    - sentences: list of strings\n",
        "  Output:\n",
        "    - BERT-like embeddings: tf.Tensor of shape=(len(sentences), 768)\n",
        "  '''\n",
        "  preprocessed_text = preprocessor(sentences)\n",
        "  return encoder(preprocessed_text)['pooled_output']"
      ],
      "metadata": {
        "id": "9Ce4XkqUEWKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_embeddings([\n",
        "    \"Two men in green shirts are standing in a yard .\"]\n",
        ")"
      ],
      "metadata": {
        "id": "vea2c8QsEZD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "x = preprocessor(i)\n",
        "x = encoder(x)\n",
        "x = tf.keras.layers.Dropout(0.2, name=\"dropout\")(x['pooled_output'])\n",
        "x = tf.keras.layers.Dense(100, activation='softmax', name=\"output\")(x)\n",
        "model = tf.keras.Model(i, x)"
      ],
      "metadata": {
        "id": "JOsWG0udGOi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "\n",
        "METRICS = [\n",
        "      tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),  \n",
        "]\n",
        "\n",
        "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", \n",
        "                                                      patience = 3,\n",
        "                                                      restore_best_weights = True)\n",
        "\n",
        "model.compile(optimizer = \"adam\",\n",
        "              loss = \"categorical_crossentropy\",\n",
        "              metrics = METRICS)\n",
        "\n",
        "model_fit = model.fit(x_train, \n",
        "                      y_train, \n",
        "                      epochs = n_epochs,\n",
        "                      validation_data = (x_test, y_test),\n",
        "                      callbacks = [earlystop_callback],\n",
        "                      shuffle=True)"
      ],
      "metadata": {
        "id": "omhELDgcGTDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = list(range(1, n_epochs+1))\n",
        "metric_list = list(model_fit.history.keys())\n",
        "num_metrics = int(len(metric_list)/2)\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=num_metrics, figsize=(30, 5))\n",
        "\n",
        "for i in range(0, num_metrics):\n",
        "  ax[i].plot(x, model_fit.history[metric_list[i]], marker=\"o\", label=metric_list[i].replace(\"_\", \" \"))\n",
        "  ax[i].plot(x, model_fit.history[metric_list[i+num_metrics]], marker=\"o\", label=metric_list[i+num_metrics].replace(\"_\", \" \"))\n",
        "  ax[i].set_xlabel(\"epochs\",fontsize=14)\n",
        "  ax[i].set_title(metric_list[i].replace(\"_\", \" \"),fontsize=20)\n",
        "  ax[i].legend(loc=\"lower left\")"
      ],
      "metadata": {
        "id": "zfNYXjNOGYqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp33.head(30)"
      ],
      "metadata": {
        "id": "beLtyzCpHGT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = [\n",
        "    \"two men cooking a meal\",\n",
        "    \"two men stove preparing food\",\n",
        "    \"two guys\",\n",
        "    \"two men\"\n",
        "]\n",
        "\n",
        "def predict_class(reviews):\n",
        "  '''predict class of input text\n",
        "  Args:\n",
        "    - reviews (list of strings)\n",
        "  Output:\n",
        "    - class (list of int)\n",
        "  '''\n",
        "  return [np.argmax(pred) for pred in model.predict(reviews)]\n",
        "\n",
        "\n",
        "predict_class(reviews)"
      ],
      "metadata": {
        "id": "gcGYwhytHPyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp[tmp['label'] == 77]"
      ],
      "metadata": {
        "id": "B8emQreQKMng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model predicts two sentences correctly. The two that are correct are th sentences that match word for word. However on the two it mislabels we see that it still predicts a label with two men present. How can we improve??"
      ],
      "metadata": {
        "id": "zm6QEDUYKCaF"
      }
    }
  ]
}