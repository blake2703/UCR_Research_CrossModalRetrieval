{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea8b4ed5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/blakedickerson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/blakedickerson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/blakedickerson/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.NeededModules import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78370622",
   "metadata": {},
   "source": [
    "## Comment Model Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb8e4a8",
   "metadata": {},
   "source": [
    "## Section 2 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f4565da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_columns(dataframe):\n",
    "    \"\"\"\n",
    "        This function will clean the whitespace out of the column names\n",
    "        \n",
    "        args:\n",
    "            dataframe: pandas dataframe\n",
    "    \"\"\"\n",
    "    dataframe.columns = dataframe.columns.str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5299dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comment_number_column(comment_number):\n",
    "    \"\"\"\n",
    "        This function will strip the whitespace to the left of each number in the comment number column\n",
    "        \n",
    "        args:\n",
    "            comment_number: object\n",
    "        \n",
    "        return:\n",
    "            comment_number: object\n",
    "    \"\"\"\n",
    "    for number in comment_number:\n",
    "        comment_number = comment_number.lstrip()\n",
    "    return comment_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a29997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_null(df):\n",
    "    \"\"\"\n",
    "        This function will clean the null value in our dataset\n",
    "        \n",
    "        args:\n",
    "            df: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    #Filter location to where the data is missing a value\n",
    "    df[df['image_name'] == '2199200615.jpg']\n",
    "    #Create a temporary variable to hold that row\n",
    "    tmp = df.iloc[19999]\n",
    "    #Get the comment number column\n",
    "    holder = tmp['comment_number']\n",
    "    #Seperate the text from the digit and print\n",
    "    result = ''.join([i for i in holder if not i.isdigit()])\n",
    "    #Update the temporary variable row\n",
    "    tmp['comment'] = result \n",
    "    tmp['comment_number'] = '4'\n",
    "    #Update our data copy row to reflect change\n",
    "    df.iloc[19999] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a836a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(comment):\n",
    "    \"\"\"\n",
    "        This function will expand all contractions to get rid of them.\n",
    "\n",
    "        args:\n",
    "            comment (object): A column in the dataframe that contains all the comments from each image\n",
    "\n",
    "        return:\n",
    "           comment: A fixed object that has all the punctuations taken out of the sentence\n",
    "    \"\"\"\n",
    "    comment = contractions.fix(comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "129cd502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(comment):\n",
    "    \"\"\"\n",
    "        This function will go through all the comment rows and remove all punctuations.\n",
    "    \n",
    "        args:\n",
    "            comment (object): A column in the dataframe that contains all the comments from each image\n",
    "\n",
    "        return:\n",
    "            comment: A fixed object that has all the punctuations taken out of the sentence\n",
    "    \"\"\"\n",
    "    for letter in string.punctuation:\n",
    "        comment = comment.replace(letter, '')\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6a05a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(comment):\n",
    "    \"\"\"\n",
    "        This function will convert a given text to lower cae\n",
    "        \n",
    "        args: \n",
    "            comment: object\n",
    "        \n",
    "        return:\n",
    "            comment: object\n",
    "            \n",
    "    \"\"\"\n",
    "    for word in comment:\n",
    "        comment = comment.lower()\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e8cdf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_whitespace(df, column_name):\n",
    "    \"\"\"\n",
    "        This function will strip the whitespace off a given column\n",
    "        \n",
    "        args:\n",
    "            df: pandas dataframe\n",
    "            column_name: object\n",
    "        \n",
    "        return: \n",
    "            df: pandas dataframe\n",
    "    \"\"\"\n",
    "    df[column_name] = df[column_name].str.replace('\\s+', '', regex=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9b389c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_images(dataframe, column_name, new_column_name):\n",
    "    \"\"\"\n",
    "        A function to turn the images to a label classifying them\n",
    "\n",
    "        args: \n",
    "            df: a dataframe\n",
    "            column_name: object\n",
    "            new_column_name: oject    \n",
    "     \"\"\"\n",
    "    dic = dict()                             #dictionary to hold our unique labels\n",
    "    label_list = list()                      #list to hold all of the labels\n",
    "    dataframe['label'] = np.nan              #create a new column which will be our label column\n",
    "    temp_counter = 0                         #temporary counting variable\n",
    "    for value in dataframe[column_name]:\n",
    "        if value not in dic:\n",
    "            dic[value] = temp_counter        #assign the counter as the value while the image_name is the key\n",
    "            temp_counter += 1\n",
    "    images = dataframe[column_name].tolist() #put the image_name column values in a list\n",
    "    for image in images:\n",
    "        label_list.append(dic[image])\n",
    "    dataframe[new_column_name] = label_list  #update label column to the correct value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbaf370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(comment):\n",
    "    \"\"\"\n",
    "        This function will count the amount of words in each comment\n",
    "\n",
    "        args:\n",
    "            comment (object): A column in the dataframe that contains all the comments from each image\n",
    "\n",
    "        returns:\n",
    "            int: An int that counts the words in the sentence\n",
    "    \"\"\"\n",
    "    return len(comment.split()) #split the string on each space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2a2b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_words(comment):\n",
    "    \"\"\"\n",
    "        This function will count the amount of unique words in a comment\n",
    "\n",
    "        args:\n",
    "            comment (object): A column in the dataframe that contains all the comments from each image\n",
    "\n",
    "        returns:\n",
    "            Object: A fixed object that has all the punctuations taken out of the sentence\n",
    "    \"\"\"\n",
    "    return len(set(comment.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56391b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_words_vs_word_count(num_words, num_unique):\n",
    "    \"\"\"\n",
    "        This function will count the number of unique words vs the total words per sentence\n",
    "\n",
    "        args:\n",
    "            num_words (int): number of words per sentence\n",
    "            num_unique (int): number of unique words per sentence\n",
    "\n",
    "        returns:\n",
    "            Float: the ratio of unique words to total words\n",
    "    \"\"\"\n",
    "    return num_unique/num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d43955",
   "metadata": {},
   "source": [
    "## Section 3 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c2f2b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Xids(num_samples, seq_len):\n",
    "    \"\"\"\n",
    "        This function will generate a numpy array in a 2d shape\n",
    "        \n",
    "        args:\n",
    "            num_samples (int): length of a pandas dataframe\n",
    "            seq_len (int): max number of words to parse\n",
    "        \n",
    "        return:\n",
    "            Xids: numpy array\n",
    "    \"\"\"\n",
    "    Xids = np.zeros((num_samples, seq_len))\n",
    "    return Xids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7c41400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_attn_msks(num_samples, seq_len):\n",
    "    \"\"\"\n",
    "        This function will generate a numpy array in a 2d shape\n",
    "        \n",
    "        args:\n",
    "            num_samples (int): length of a pandas dataframe\n",
    "            seq_len (int): max number of words to parse\n",
    "        \n",
    "        return:\n",
    "            attn_masks: numpy array\n",
    "    \"\"\"\n",
    "    attn_masks = np.zeros((num_samples, seq_len))\n",
    "    return attn_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d494b26",
   "metadata": {},
   "source": [
    "# Fix function below to take in any column instead of comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db88b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_encoded_values(df, ids, masks, tokenizer, sequence_len, column):\n",
    "    \"\"\"\n",
    "        This function will encode each comment 1 by 1\n",
    "        \n",
    "        args:\n",
    "            df: pandas dataframe\n",
    "            ids: Xids\n",
    "            masks: attn_mask\n",
    "            tokenizer: huggingface AutoTokenizer\n",
    "            \n",
    "        return:\n",
    "            ids: input ids to pass to BERT model\n",
    "            masks: attention masks to pass to BERT model\n",
    "    \"\"\"\n",
    "    for i, text in tqdm(enumerate(df['comment'])):\n",
    "        tokenized_text = tokenizer.encode_plus(\n",
    "                              text,\n",
    "                              max_length=sequence_len,      #max length of each comment so we can truncate if needed\n",
    "                              truncation=True,              #if the comment is longer than max_length only take up to max_length values\n",
    "                              padding='max_length',         #make all elements the same size of max_length\n",
    "                              add_special_tokens=True,      #add [cls] [pad] [sep]\n",
    "                              return_tensors='tf'           #return the data as tensorflow tensors\n",
    "        )\n",
    "        ids[i, :] = tokenized_text.input_ids          #at current index to the end tokenize the input ids\n",
    "        masks[i, :] = tokenized_text.attention_mask   #at current index to the end tokenize the mask\n",
    "    return ids, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daffd23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_encoded_values_dynamically(df, ids, masks, tokenizer, sequence_len, column):\n",
    "    \"\"\"\n",
    "        This function will encode each comment 1 by 1\n",
    "        \n",
    "        args:\n",
    "            df: pandas dataframe\n",
    "            ids: Xids\n",
    "            masks: attn_mask\n",
    "            tokenizer: huggingface AutoTokenizer\n",
    "            \n",
    "        return:\n",
    "            ids: input ids to pass to BERT model\n",
    "            masks: attention masks to pass to BERT model\n",
    "    \"\"\"\n",
    "    for i, text in tqdm(enumerate(df[column])):\n",
    "        tokenized_text = tokenizer.encode_plus(\n",
    "                              text,\n",
    "                              max_length=sequence_len,      #max length of each comment so we can truncate if needed\n",
    "                              truncation=True,              #if the comment is longer than max_length only take up to max_length values\n",
    "                              padding='max_length',         #make all elements the same size of max_length\n",
    "                              add_special_tokens=True,      #add [cls] [pad] [sep]\n",
    "                              return_tensors='tf'           #return the data as tensorflow tensors\n",
    "        )\n",
    "        ids[i, :] = tokenized_text.input_ids          #at current index to the end tokenize the input ids\n",
    "        masks[i, :] = tokenized_text.attention_mask   #at current index to the end tokenize the mask\n",
    "    return ids, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46abb10",
   "metadata": {},
   "source": [
    "## Section 5 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdf36115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_loss_plots(num_epochs, model_name):\n",
    "    \"\"\"\n",
    "        This function will plot the training loss and training accuracy over a fixed amount of epochs\n",
    "        \n",
    "        args:\n",
    "            num_epochs (int): number of epochs trained for\n",
    "            model_name (object): name of the model trained\n",
    "    \"\"\"\n",
    "    x = list(range(1, num_epochs+1))\n",
    "    metric_list = list(model_name.history.keys())\n",
    "    num_metrics = int(len(metric_list)/2)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=num_metrics, figsize=(30,5))\n",
    "    \n",
    "    for i in range(0, num_metrics):\n",
    "        ax[i].plot(x, model_name.history[metric_list[i]], marker=\"o\", label=metric_list[i].replace(\"_\", \" \"))\n",
    "        ax[i].plot(x, model_name.history[metric_list[i+num_metrics]], marker=\"o\", label=metric_list[i+num_metrics].replace(\"_\", \" \"))\n",
    "        ax[i].set_xlabel(\"epochs\",fontsize=14)\n",
    "        ax[i].set_title(metric_list[i].replace(\"_\", \" \"),fontsize=20)\n",
    "        ax[i].legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590dfdaa",
   "metadata": {},
   "source": [
    "## Section 6 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d42402cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_num_classes(num_classes):\n",
    "    \"\"\"\n",
    "        This function will generate a list of the number of classes (labels) generated in the dataset\n",
    "        \n",
    "        args:\n",
    "            num_classes (int): number of classes generated\n",
    "        \n",
    "        return:\n",
    "            class_list (int): a list of all the classes\n",
    "    \"\"\"\n",
    "    class_list = []\n",
    "    for i in range(0, num_classes):\n",
    "        class_list.append(i)\n",
    "    return class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44527380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(text, tokenizer, seq_len):\n",
    "    '''\n",
    "        This function will encode a given string\n",
    "        \n",
    "        args:\n",
    "            text: a string input\n",
    "            tokenizer: 'bert-base-cased' tokenizer that we used previously\n",
    "        \n",
    "        return:\n",
    "            input_ids: tf.float64\n",
    "            attention_mask: tf.float64\n",
    "    '''\n",
    "    token = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=seq_len, \n",
    "        truncation=True, \n",
    "        padding='max_length', \n",
    "        add_special_tokens=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': tf.cast(token.input_ids, tf.float64),\n",
    "        'attention_mask': tf.cast(token.attention_mask, tf.float64)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdfe87d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, processed_data, classes):\n",
    "    '''\n",
    "        This funciton will determine the label for a given string inputted\n",
    "        \n",
    "        args:\n",
    "            model: BERT model that was created\n",
    "            processed_data: tokenized data\n",
    "            classes: class list\n",
    "        \n",
    "        return:\n",
    "            the index of the largest value predicted\n",
    "    '''\n",
    "    probs = model.predict(processed_data)[0]\n",
    "    return classes[np.argmax(probs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3571cf70",
   "metadata": {},
   "source": [
    "## Noisy Tag Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe545e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(df, \n",
    "                     column_name, \n",
    "                     language='en', \n",
    "                     max_ngram_size=3, \n",
    "                     dedupLim=0.9, \n",
    "                     dedupFunc='seqm', \n",
    "                     num_keywords=5):\n",
    "    '''\n",
    "        This function will use the yake library to grab the highest weighted tags from each comment\n",
    "        \n",
    "        args:\n",
    "            df: pandas dataframe\n",
    "            column_name: the name of the column needed to extract information from\n",
    "            language: english\n",
    "            max_ngram_size: maximum number of n-grams to consider\n",
    "            dedupLim: threshold cutoff\n",
    "            dedupFunc: algorithm specification\n",
    "            num_keywords: number of keywords to extract\n",
    "            \n",
    "        return:\n",
    "            pandas dataframe\n",
    "    '''\n",
    "    \n",
    "    kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, top=num_keywords) #create the keyword extractor object with paramerts\n",
    "    keyword_counts = []\n",
    "    for sentence in df[column_name]:\n",
    "        keywords = kw_extractor.extract_keywords(sentence) #extract the keywords from the sentence\n",
    "        keyword_counts.append(keywords)                    #add the keywords to the list\n",
    "    df['keywords'] = keyword_counts                        #create a new column and add the 'noisy tags'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6e46a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nouns_verbs(df, column_name):\n",
    "    '''\n",
    "        This function will use the NLTK library to tag the nouns and verbs from each commment and put them in a new column\n",
    "        \n",
    "        args:\n",
    "            df: pandas dataframe\n",
    "            column_name: the name of the column needed to extract information from\n",
    "\n",
    "        return:\n",
    "            pandas dataframe\n",
    "    '''\n",
    "        \n",
    "    nouns_verbs = []\n",
    "    for sentence in df[column_name]:\n",
    "        tokens = nltk.word_tokenize(sentence) #tokenize each word in the sentence\n",
    "        tagged = nltk.pos_tag(tokens)         #part of speech tagging\n",
    "        nv = [word for (word, pos) in tagged if pos.startswith('N') or pos.startswith('V')] #get the nouns and verbs from the tagged data\n",
    "        nouns_verbs.append(nv) #add nouns and verbs to list\n",
    "    df['noisy_tags'] = nouns_verbs\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ae0761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_noisy_tags(df, column_name):\n",
    "    df[column_name] = [','.join(map(str, word)) for word in df[column_name]]\n",
    "    df[column_name] = df[column_name].str.replace(',', ' ')\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
